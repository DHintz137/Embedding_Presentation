---
webr: 
  show-startup-message: false  
  packages: ['tibble', 'dplyr', 'feather']
format:
  revealjs:
    include-in-header:
      - file: js_scripts.html
    theme: [default, theme.scss]
    scrollable: true
    css: styles.css
    transition: slide
    highlight-style: github
    slide-number: true
    sc-sb-title: true
    logo: images/quarto.png
    footer-logo-link: "https://quarto.org"
    chalkboard: true
    resources:
      - British_National_Corpus_Vector_size_300_skipgram/model.bin
bibliography: references.bib
csl: diabetologia.csl
filters: 
 - webr
 - reveal-header
slide-level: 3
number-sections: false
engine: knitr
---

# [An Exploration of Information Loss in Transformer Embedding Spaces for Enhancing Predictive AI in Genomics]{
  style="color:#FFFFFF; 
         font-size: 55px; 
         position: relative;
         bottom: 60px; 
         text-shadow: -1px -1px 0 #000, 
                      1px -1px 0 #000, 
                      -1px 1px 0 #000, 
                      1px 1px 0 #000;"
}{background-image="images/DALL_E_DNA_Image_edited.png" background-size="cover" background-color="#4f6952"}


<h1 style="color:#FFFFFF; 
         font-size: 28px; 
         text-align: center;
         position: absolute; 
         top: 325px; 
         width: 100%; 
         text-shadow: -0.75px -0.75px 0 #000, 
                      0.75px -0.75px 0 #000, 
                      -0.75px 0.75px 0 #000, 
                      0.75px 0.75px 0 #000;">Daniel Hintz <br> 2024-06-06</h2>

# [Outline]{style="font-size: 55px;"} 

::: {style="font-size: 0.65em;"}
- Thesis Question
- Background
    - DNA
    - Word Embeddings
    - GenSLM Embedding Algorithm 
- Data and Processing
    - Source and Cleaning 
- Methodology
    - Intrinsic and Extrinsic Evaluation
- Results
:::


## Thesis Question

. . .

::: {style="text-align:center; font-size: 0.85em;"}
How easily can information be extracted from GenSLM embeddings while maintaining its original integrity; and what is the quality of the produced vector embeddings?
:::



# [Background]{style="color:white;"}{background-color="#8FA9C2"}

## DNA

::: columns
::: {.column width="80%"}
::: incremental
::: {style="font-size: 0.6em;"}
- Deoxyribonucleic acid (DNA) is a molecule that contains the genetic code that provides the instructions for building and maintaining life.
- The structure of DNA can be thought of as rungs on a ladder (known as base pairs) involving the pairing of four nucleotides - Adenine (A), Cytosine (C),Guanine (G) and Thymine (T).
- Genomic sequencing is a process used to decipher the genetic material found in an organism
- For reference, the SARS-CoV2 virus is approximately 30,0000 base pairs (bp’s), whereas the human genome is approximately 3 billion bp.
:::
:::
:::
::: {.column width="20%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 1: DNA Base Pairs; [@NHGRI2024]](images/DNA_diagram.png)
:::
:::
:::

## DNA Sequencing Technology

::: columns
::: {.column width="50%"}
::: incremental
::: {style="font-size: 0.6em;"}
- The two main technologies for DNA Sequencing is Ilumina and Nanopore
- Big Picture, the workflow for sequencing is the same for each technology:
    - Someone gets tested for Covid, PCA is ran to detect if the assay is indeed positive
    - Positive tests sequence using a machine that is most likely either Ilumina or Oxford Nanopore 
    - This takes in the Covid Sample and arrives at a digital copy of DNA sequences
:::
:::
:::
::: {.column width="50%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 2: Simplified Schema of Sequencing Process for SARS-CoV-2](images/DNA_sequencing.png)
:::
:::
:::


## Coding regions and Proteins

::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- The coding region specifically encodes proteins that are essential for all the functions necessary for life.
- the whole SARS-CoV2 genome, marking structural and nonstructural proteins within the coding region along with the first 43 nucleotides for the nsp1 protein
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 3: SARS-CoV-2 Proteins; Adapted from [@kandwal2023genetic], pg. 99](images/nsp1_str_non_str.png)
:::
:::
:::

## Word Embeddings (1)

::: incremental
::: {style="font-size: 0.6em;"}
- Before we introduce GenSLM and embeddings for Sequences, what is an embedding and why would ever embed something? 
- If we consider English text, how might we ever measure the similarity of words in terms of their meaning, said differently, **How can we measure semantic similarity between words**
- For example, what is the semantic difference between the words "King" and "Woman"
- This initially sounds outlandish, but if there were a way to represent words as numeric vectors, then we would be able to use the same linear algebra we would use to compare vectors of force, speed or velocity. 
:::
:::

## Word Embeddings  (2)
::: incremental
::: {style="font-size: 0.6em;"}
- Here is where neural network embedding algorithms enters
- For example, given a corpus (a massive training set of structured text) how are the words "King", "Quen", "Man", "Woman" related within the given corpus
- It turns out given a sufficiently large corpus of text you can train an embedding algorithm (such as word2vec for NLP) to generate vectors for words included within the corpus 
:::
:::

. . .

```{webr-r}
#| context: setup

options(max.print = 15)
# Download a dataset
url <- "https://raw.githubusercontent.com/DHintz137/Embedding_Presentation/main/raw-data/King_Man_Woman_Queen_pred.csv"
download.file(url, "King_Man_Woman_Queen_pred.csv")
embedding_raw <- read.csv("King_Man_Woman_Queen_pred.csv")
embedding <- t(data.frame(embedding_raw[,-1] ,row.names = embedding_raw[,1]))
colnames(embedding) <- c("King", "Man", "Woman", "Queen")
# embedding <- as_tibble(embedding)

print_tidy <- function(x, ...) {
  if (is.matrix(x) || length(dim(x)) > 1) {
    # If x is a matrix or array, convert each column into a tibble column
    out <- as_tibble(as.data.frame(x))
  } else if (is.vector(x)) {
    # If x is a vector, convert it into a single column tibble
    out <- tibble(value = x)
  } else {
    # Fallback for other data types
    out <- as_tibble(x)
  }
  print(out, ...)
}
```


```{r, echo=TRUE, eval=FALSE}
## Pre-Ran Code ##
library(word2vec)
# using British National Corpus http://vectors.nlpl.eu/repository/
model <- read.word2vec("British_National_Corpus_Vector_size_300_skipgram", normalize = TRUE)
embedding <- predict(model,newdata = c("king_NOUN","man_NOUN","woman_NOUN","queen_NOUN"),type="embedding")
```

::: {style="margin-top: 20px;"}
:::

. . .

```{webr-r}
print_tidy(embedding)
print_tidy(embedding[ ,"King"] - embedding[ , "Man"] + embedding[ , "Woman"])
print_tidy(t(embedding[ , "Man"]) %*% embedding[ , "Woman"])
```

## Word Embeddings (3)
::: {style="font-size: 0.6em;"}
- We can Plot the vectors shown in the previous slide to demonstrate how word2vec produces embeddings that measures semantic similarity between words 
:::

. . .

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 4: Word Embedding Vectors ](images/King_Man_Woman_Queen.png){width="70%"}
:::


## Sequence Representations (1)

::: incremental
::: {style="font-size: 0.6em;"}
- Just like we can represent words as vectors we can also represent sequences as vectors
- There exist many neural network embedding algorithms for genomic data
- For example, **GenSLM** (the focus of this presentation), DNABERT2, and HyenaDNA
- These embeddings are lossy encodings, meaning that some information is lost in the transformation
- They also can distort structural relationships present in the data
:::
:::

## Sequence Representations (2)

::: incremental
::: {style="font-size: 0.6em;"}
- Thus, embedding evaluation is a rising field as embeddings are increasingly used in pipelines for predictive analytics and data discovery
- How data is represented is very important!!
:::
:::

::: {style="margin-top: 20px;"}
:::

. . .

::: {style="text-align:center; font-size: 0.5em;"}
![](images/ml_rep_obj.png){width="80%"}
:::

::: {style="margin-top: 20px;"}
:::

::: incremental
::: {style="font-size: 0.6em;"}
-  As seen in the equation above, an effective representation (embedding) of a genomic sequence is crucial for all downstream tasks
    - i.e., clustering, classification, regression, protein function identification, structural analysis, and predicting genetic disorders [@angermueller2016deep]
- While there are many neural network embedding algorithms, the focus of this presentation is on GenSLM 
:::
:::

## GenSLM

::: panel-tabset
### Overview

::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- Overall, the GenSLM algorithm first **tokenizes**, i.e., breaks up sequences in to chunks of three nucleotides 
- Then, input sequence are **vectorized** creating a $1 \times 512$ vector for each imputed sequence
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Generic Embedding Workflow](images/Tokenize_vectorize.png){width="50%"}
:::
:::
:::

### Transformer
::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- First the tokenized input sequence is passed to the transformer encoder
- The transformer encoder converts 1,024 bp slices into numeric vectors 
- This is ran recursively through a diffusion model to learn a condensed distribution of the whole sequence.
- The transformers in GenSLM are used to capture local interactions within a genomic sequence
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 6: GenSLM Transformer Architecture; [@genslms2023]](images/GenSLM.png)
:::
:::
:::

### Diffusion  + Transformer



::: incremental
::: {style="font-size: 0.6em;"}
- Diffusion and Transformer models work in tandem in GenSLM
- Transformers captures local interactions, while the diffusion model captures learns a representation of transformers vectors to represent the whole genome
:::
:::


:::

# [Workflow]{style="color:white;"}{background-color="#8FA9C2"}
## Overview 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 7: Project Workflow](images/Overall_workflow.png){width="30%"}
:::

# [Data and Processing]{style="color:white;"}{background-color="#8FA9C2"}

## Data Description


::: incremental
::: {style="font-size: 0.6em;"}
- All Covid sequences were downloaded with permission from the Global Initiative on Sharing All Influenza Data (GISAID)
- API GISAIDR was used for convenience
- No geographical or temporal restrictions were placed on the sequences extracted
:::
:::

## Data Cleaning 

::: incremental
::: {style="font-size: 0.6em;"}
- GISAID's exclusion criteria was used to remove sequences of poor quality 
- Additional exclusion criteria was implemented to remove all sequence with ambiguous nucleotides (the `NA's` of the genomic world)
:::
:::


## Alignment 

::: incremental
::: {style="font-size: 0.6em;"}
- An multiple alignment alignment was performed to be able to slice the exact location of proteins across different sequences, for more details see the appendix
:::
::: 

## Exploratory Data Analysis

::: incremental
::: {style="font-size: 0.6em;"}
- For Exploratory Data Analysis, the goal was the characterize the structure of the data in its sequence presentation 
-  This served as a later comparisons to embedding structure 
:::
::: 

. . .

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 8: Mutation Boxplot](images/Whole_Genome_Mutation_Count_Hamming_Distances.png){width="65%"}
:::

## 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 11: Hamming Distance of Aligned Proteins to Wuhan Reference Sequence](images/hamming_prot_heatmap_581_all_variants.png){width="100%"}
:::




## 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 12: Aligned Sequence Radial Dendrogram](images/high_res_tree_595_patients_gapinc_cropped.png){width="65%"}
:::


# [Methodology]{style="color:white;"}{background-color="#8FA9C2"}

## Evaluating Embeddings 

::: incremental
::: {style="font-size: 0.6em;"}
- Broadly speaking, the quality of an embedding is assessed based upon its information richness and the degree of non-redundancy.

- Their are two main methodologies for evaluating embeddings 

- When downstream learning tasks are evaluated, it’s referred to as **extrinsic evaluation**.
- When the qualities of the embedding matrix itself are assessed it is referred to as **intrinsic evaluation** (Lavrač et al., 2021).
:::
:::

## Intrinsic Evaluation

::: incremental
::: {style="font-size: 0.6em;"}
- There are three main qualities to asses when studying the the quality of an embedding: **Redundancy**, **Separability**, **Preservation of Semantic Distance**
- Redundancy reveals the efficiency of a embeddings encoding; more efficient encodings tend to perform better and use fewer computational resources.

- Separability gives practical insight into whether or not the embedding output can be separated into meaningful genomic groups (i.e., variants). 

- Preservation of semantic distance is important for determining if the structural representation of information remains valid for subsequent tasks.
:::
:::

## Extrinsic Evaluation

::: incremental
::: {style="font-size: 0.6em;"}
- Extrinsic evaluation pursues practical benchmarks to assess the performance of an embeddings algorithm per its associated embedding matrix across varying levels of task complexity
- For GenSLM, the subset of possible tasks chosen were variant and protein classification
:::
:::

# [Results]{style="color:white;"}{background-color="#8FA9C2"}
## [Intrinsic Evaluation Results]{style="color:#6FB1D1;"}


### Redundancy

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 13: SVD CPE Plot](images/SVD_582.png){width="65%"}
:::


### Preservation of Semantic Distance

::: panel-tabset
#### Distance matrices
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 14: Distance Matrices](images/distances_matrices_labelled.png){width="65%"}
:::

#### Kullback–Leibler Divergence (KLD)
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 15: CKL vs DKL](images/CKL_vs_DKL_ver2.png){width="65%"}
:::
:::
# 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 16: Embedding Radial Dendrogram](images/high_res_tree_581_patients_emb.png){width="65%"}
:::

### Separability

## [Extrinsic Evaluation Results]{style="color:#6FB1D1;"}
### Variant Classification
### Protein Classification

## Conclusion

<iframe id="plotlyFrame" src="./plotly_p1.html" style="border: none;"></iframe>



# [Appendix]{style="color:white;"}{background-color="#8FA9C2"}

## Alignment (Appendix)

```{webr-r}
#| context: setup
options(max.print = 10)
# Download a dataset
url <- "https://raw.githubusercontent.com/DHintz137/Embedding_Presentation/main/raw-data/var_seq_595_A.feather"
download.file(url, "var_seq_595_A.feather")
var_seq_595_A <- read_feather("var_seq_595_A.feather")
```


::: {style="text-align:center; font-size: 0.5em;"}
![Figure 9: Alignment Example](images/alignment.png){width="65%"}
:::


#

{{< pdf DHintz_PlanB_Submission.pdf width=1000 height=650 >}}

# `r fontawesome::fa("hand-holding-heart", "white")` [Acknowledgements]{style="color:white; font-size: 70px;"} {background-color="#8FA9C2"}

::: incremental
- Committee members 
- Cohort
:::

# [Thank you!!]{.center style="color:white;"}{background-color="#8FA9C2"}

# [References]{.center style="color:#464D58;"}{background-color="#8FA9C2"}

