---
webr: 
  show-startup-message: false  
  packages: ['tibble', 'dplyr', 'feather']
format:
  revealjs:
    include-in-header:
      - file: MathJax.html
    theme: [default, theme.scss]
    scrollable: true
    css: styles.css
    transition: slide
    highlight-style: github
    slide-number: true
    sc-sb-title: true
    logo: images/quarto.png
    footer-logo-link: "https://quarto.org"
    chalkboard: true
    resources:
      - British_National_Corpus_Vector_size_300_skipgram/model.bin
bibliography: references.bib
csl: diabetologia.csl
filters: 
 - webr
 - reveal-header
slide-level: 3
number-sections: false
engine: knitr
---

# [An Exploration of Information Loss in Transformer Embedding Spaces for Enhancing Predictive AI in Genomics]{
  style="color:#FFFFFF; 
         font-size: 55px; 
         position: relative;
         bottom: 60px; 
         text-shadow: -1px -1px 0 #000, 
                      1px -1px 0 #000, 
                      -1px 1px 0 #000, 
                      1px 1px 0 #000;"
}{background-image="images/DALL_E_DNA_Image_edited.png" background-size="cover" background-color="#4f6952"}


<h1 style="color:#FFFFFF; 
         font-size: 28px; 
         text-align: center;
         position: absolute; 
         top: 325px; 
         width: 100%; 
         text-shadow: -0.75px -0.75px 0 #000, 
                      0.75px -0.75px 0 #000, 
                      -0.75px 0.75px 0 #000, 
                      0.75px 0.75px 0 #000;">Daniel Hintz <br> 2024-06-06</h2>

# [Outline (1)]{style="font-size: 55px;"} 

::: incremental
::: {style="font-size: 0.6em;"}
-   Background
    - SARS-COV-2
    - DNA
    - Coding regions and Proteins
    - Sequence Representations
    - GenSLM
-   Data and Processing
    - Data Description
    - Data Cleaning & Pre-Processing
    - Exploratory Data Analysis
    - Generating Embeddings
-   Methodology
:::
:::



# [Background]{style="color:white;"}{background-color="#8FA9C2"}

## DNA

::: columns
::: {.column width="80%"}
::: incremental
::: {style="font-size: 0.6em;"}
- Deoxyribonucleic acid (DNA) is a molecule that contains the genetic code that provides the instructions for building and maintaining life.
- The structure of DNA can be thought of as rungs on a ladder (known as base pairs) involving the pairing of four nucleotides - Adenine (A), Cytosine (C),Guanine (G) and Thymine (T).
- Genomic sequencing is a process used to decipher the genetic material found in an organism
- For reference, the SARS-CoV2 virus is approximately 30,0000 base pairs (bp’s), whereas the human genome is approximately 3 billion bp.
:::
:::
:::
::: {.column width="20%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 1: DNA Base Pairs; [@NHGRI2024]](images/DNA_diagram.png)
:::
:::
:::

## DNA Sequencing Technology

::: columns
::: {.column width="50%"}
::: incremental
::: {style="font-size: 0.6em;"}
- The two main technologies for DNA Sequencing is Ilumina and Nanopore
- Big Picture, the workflow for sequencing is the same for each technology:
    - Someone gets tested for Covid, PCA is ran to detect if the assay is indeed positive
    - Positive tests sequence using a machine that is most likely either Ilumina or Oxford nanopore 
    - This takes in the Covid Sample and arrives at a digital copy of DNA sequences
:::
:::
:::
::: {.column width="50%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 2: Simplified Schema of Sequencing Process for SARS-CoV-2](images/DNA_sequencing.png)
:::
:::
:::


## Coding regions and Proteins

::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- The coding region specifically encodes proteins that are essential for all the functions necessary for life.
- the whole SARS-CoV2 genome, marking structural and nonstructural proteins within the coding region along with the first 43 nucleotides for the nsp1 protein
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 3: SARS-CoV-2 Proteins; Adapted from [@kandwal2023genetic], pg. 99](images/nsp1_str_non_str.png)
:::
:::
:::

## Word Embeddings (1)

::: incremental
::: {style="font-size: 0.6em;"}
- Before we introduce GenSLM and embeddings for Sequences, what is an embedding and why would ever embed something? 
- If we consider English text, how might we ever measure the similarity of words in terms of their meaning, said differently, **How can we measure semantic similarity between words**
- For example, what is the semantic difference between the words "King" and "Woman"
- This initially sounds outlandish, but if there were a way to represent words as numeric vectors, then we would be able to use the same linear algebra we would use to compare vectors of force, speed or velocity. 
:::
:::

## Word Embeddings  (2)
::: incremental
::: {style="font-size: 0.6em;"}
- Here is where neural network embedding algorithms enters
- For example, given a corpus (a massive training set of structured text) how are the words "King", "Quen", "Man", "Woman" related within the given corpus
- It turns out given a sufficiently large corpus of text you can train an embedding algorithm (such as word2vec for NLP) to generate vectors for words included within the corpus 
:::
:::

. . .

```{webr-r}
#| context: setup
options(max.print = 10)
# Download a dataset
url <- "https://raw.githubusercontent.com/DHintz137/Embedding_Presentation/main/raw-data/King_Man_Woman_Queen_pred.csv"
download.file(url, "King_Man_Woman_Queen_pred.csv")
embedding_raw <- read.csv("King_Man_Woman_Queen_pred.csv")
embedding <- t(data.frame(embedding_raw[,-1] ,row.names = embedding_raw[,1]))
colnames(embedding) <- c("King", "Man", "Woman", "Queen")
embedding <- as_tibble(embedding)
```


```{r, echo=TRUE, eval=FALSE}
## Pre-Ran Code ##
library(word2vec)
# using British National Corpus http://vectors.nlpl.eu/repository/
model <- read.word2vec("British_National_Corpus_Vector_size_300_skipgram", normalize = TRUE)
embedding <- predict(model,newdata = c("king_NOUN","man_NOUN","woman_NOUN","queen_NOUN"),type="embedding")
```

::: {style="margin-top: 20px;"}
:::

. . .

```{webr-r}
embedding
embedding[ ,"King"] - embedding[ , "Man"] + embedding[ , "Woman"]
embedding[ ,"Queen"]
```

## Word Embeddings (3)

- We can Plot the vectors shown in the previous slide to demonstrate how word2vec produces embeddings that measures semantic similarity between words 

. . .

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 4](images/King_Man_Woman_Queen.png)
:::


## Sequence Representations


::: {style="font-size: 0.6em;"}
- Just like we can represent words as vectors we can also represent sequences as vectors
:::

::: {style="margin-top: 20px;"}
:::

. . .

::: {style="text-align:center; font-size: 0.5em;"}
![](images/ml_rep_obj.png)
:::

::: {style="margin-top: 20px;"}
:::

::: incremental
::: {style="font-size: 0.6em;"}
-  As seen in the equation above, an effective representation (embedding) of a genomic sequence is crucial for all downstream tasks
    - i.e., clustering, classification, regression, protein function identification, structural analysis, and predicting genetic disorders [@angermueller2016deep]
- While there are many neural network embedding algorithms, the focus of this presentation is on GenSLM 
:::
:::

## GenSLM

::: panel-tabset
### Overview

::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- Overall, the GenSLM algorithm first **tokenizes**, i.e., breaks up sequences in to chunks of three nucleotide 
- Then, **vectorizes** the input sequence creating a $1 \times 512$ vector for each imputed sequence
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 3](images/Tokenize_vectorize.png){width="50%"}
:::
:::
:::

### Transformer
::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- First the tokenized input sequence is passed to the transformer encoder
- The transformer encoder converts 1,024 bp slices into numeric vectors 
- This is ran recursively through a diffusion model to learn a condensed distribution of the whole sequence.
- The transformers in GenSLM are used to capture local interactions within a genomic sequence
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: GenSLM Transformer Architecture; [@genslms2023]](images/GenSLM.png)
:::
:::
:::

### Diffusian + Transformer



::: incremental
::: {style="font-size: 0.6em;"}
- Diffusion and Transformer models work in tandem
- Transformers captures local interactions, while the diffusion model captures learns a representation of transformers vectors to represent the whole genome
:::
:::


:::

# [Workflow]{style="color:white;"}{background-color="#8FA9C2"}
## Overview 


::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- foo
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Workflow](images/Overall_workflow.png){width="45%"}
:::
:::
:::
# [Data and Processing]{style="color:white;"}{background-color="#8FA9C2"}

## Data Description


::: incremental
::: {style="font-size: 0.6em;"}
- All Covid sequences were downloaded with permission from the Global Initiative on Sharing All Influenza Data (GISAID)
- API GISAIDR was used for convenience
- No geographical or temporal restrictions were placed on the sequences extracted
:::
:::

## Data Cleaning 

::: incremental
::: {style="font-size: 0.6em;"}
- GISAID's exclusion criteria was used to remove sequences of poor quality 
- Additional exclusion criteria was implemented to remove all sequence with ambiguous nucleotides (the `NA's` of the genomic world)
:::
:::

## Alignment

::: incremental
::: {style="font-size: 0.6em;"}
- Sequences need to be aligned in order to find nucleotides responsible for coding proteins across multiple sequences 
- The Clustal Omega algorithm was used from the msa package in R (Sievers et al., 2011; Bodenhofer et al., 2015). 
- To obtain the locations of proteins in terms of base pairs, the Wuhan reference sequence (NC_045512.2), was included in the alignment for which protein locations are documented (NCBI Reference Sequence, 2023; Bai et al., 2022, p. 283). 
- Protein locations for the reference sequence
- Using these locations you can string matching unaligned reference sequences to the aligned reference sequence to find the updated protein locations 
- Now specific proteins can be selected and are reading for embedding
:::
::: 


::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Radial Dendrogram](images/Whole_Genome_Mutation_Count_Hamming_Distances.png){width="65%"}
:::



```{webr-r}
#| context: setup
options(max.print = 10)
# Download a dataset
# url <- "https://raw.githubusercontent.com/DHintz137/Embedding_Presentation/main/raw-data/var_seq_595_A.feather"
# download.file(url, "var_seq_595_A.feather")
# var_seq_595_A <- read_feather("var_seq_595_A.feather")
# var_seq_595_A$aligned_seq[c(1,2,4,78)] |> substr(640,745) |> Biostrings::DNAStringSet()
```


## Exploratory Data Analysis

::: incremental
::: {style="font-size: 0.6em;"}
- For Exploratory Data Analysis, the goal was the characterise the structure of the data
-  This served as a later compasrions to struture as seen through the embedding
:::
::: 

. . .

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Radial Dendrogram](images/Whole_Genome_Mutation_Count_Hamming_Distances.png){width="65%"}
:::

## 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Radial Dendrogram](images/hamming_prot_heatmap_581_all_variants.png){width="100%"}
:::




## 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Radial Dendrogram](images/high_res_tree_595_patients_gapinc_cropped.png){width="65%"}
:::


# [Methodology]{style="color:white;"}{background-color="#8FA9C2"}

## Goals 

This study aimed to assess the quality of the GenSLM embedding algorithms when applied to genomic analyses. The quality of an embedding is assessed based upon its information richness and the degree of non-redundancy.

When downstream learning tasks are evaluated, it’s often referred to as extrinsic evaluation. While when the qualities of the embedding matrix itself are assessed it is often referred to as intrinsic evaluation (Lavrač et al., 2021).

## Intrinsic Evaluation

Studying an embedding’s redundancy reveals the efficiency of its encoding; more efficient encodings tend to perform better and use fewer computational resources. Separability gives practical insight into whether or not the embedding output can be separated into meaningful genomic groups (i.e., variants). Meanwhile, exploring the preservation of semantic distance is important for determining if the structural representation of information remains valid for subsequent tasks.

## Extrinsic Evaluation

# [Results]{style="color:white;"}{background-color="#8FA9C2"}
## [Intrinsic Evaluation Results]{style="color:#6FB1D1;"}
### Redundancy

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Radial Dendrogram](images/SVD_582.png){width="65%"}
:::



### Preservation of Semantic Distance

::: panel-tabset
#### Distance matrices
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Radial Dendrogram](images/distances_matrices_labelled.png){width="65%"}
:::

#### Kullback–Leibler Divergence (KLD)
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Radial Dendrogram](images/CKL_vs_DKL_ver2.png){width="65%"}
:::
:::
# 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Radial Dendrogram](images/high_res_tree_581_patients_emb.png){width="65%"}
:::

### Separability

## [Extrinsic Evaluation Results]{style="color:#6FB1D1;"}
### Variant Classification
### Protein Classification

# [Appendix]{style="color:white;"}{background-color="#8FA9C2"}

#

{{< pdf DHintz_PlanB_Submission.pdf width=1000 height=650 >}}

# `r fontawesome::fa("hand-holding-heart", "white")` [Acknowledgements]{style="color:white; font-size: 70px;"} {background-color="#8FA9C2"}

::: incremental
- Committee members 
- Cohort
:::

# [Thank you!!]{.center style="color:white;"}{background-color="#8FA9C2"}

# [References]{.center style="color:#464D58;"}{background-color="#8FA9C2"}

