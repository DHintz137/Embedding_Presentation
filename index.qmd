---
webr: 
  show-startup-message: false  
  packages: ['tibble', 'dplyr', 'feather']
format:
  revealjs:
    include-in-header:
      - file: js_scripts.html
    theme: [default, theme.scss]
    scrollable: true
    css: styles.css
    transition: slide
    highlight-style: github
    slide-number: true
    sc-sb-title: true
    logo: images/quarto.png
    footer-logo-link: "https://quarto.org"
    chalkboard: true
    resources:
      - British_National_Corpus_Vector_size_300_skipgram/model.bin
bibliography: references.bib
csl: diabetologia.csl
filters: 
 - webr
 - reveal-header
slide-level: 4
number-sections: false
engine: knitr
---

# [An Exploration of Information Loss in Transformer Embedding Spaces for Enhancing Predictive AI in Genomics]{
  style="color:#FFFFFF; 
         font-size: 55px; 
         position: relative;
         bottom: 60px; 
         text-shadow: -1px -1px 0 #000, 
                      1px -1px 0 #000, 
                      -1px 1px 0 #000, 
                      1px 1px 0 #000;"
}{background-image="images/DALL_E_DNA_Image_edited.png" background-size="cover" background-color="#4f6952"}


<h1 style="color:#FFFFFF; 
         font-size: 28px; 
         text-align: center;
         position: absolute; 
         top: 325px; 
         width: 100%; 
         text-shadow: -0.75px -0.75px 0 #000, 
                      0.75px -0.75px 0 #000, 
                      -0.75px 0.75px 0 #000, 
                      0.75px 0.75px 0 #000;">Daniel Hintz <br> 2024-06-06</h2>

# [Outline]{style="font-size: 55px;"} 

::: {style="font-size: 0.65em;"}
- Thesis Question
- Background
    - DNA
    - Word Embeddings
    - GenSLM Embedding Algorithm 
- Data and Processing
    - Source and Cleaning 
- Methodology
    - Intrinsic and Extrinsic Evaluation
- Results
- Conclusion
:::


## Thesis Question

. . .

::: {style="text-align:center; font-size: 0.85em;"}
How easily can information be extracted from GenSLM embeddings while maintaining its original integrity; and what is the quality of the produced vector embeddings?
:::



# [Background]{style="color:white;"}{background-color="#8FA9C2"}

## DNA

::: columns
::: {.column width="80%"}
::: incremental
::: {style="font-size: 0.6em;"}
- Deoxyribonucleic acid (DNA) is a molecule that contains the genetic code that provides the instructions for building and maintaining life.
- The structure of DNA can be thought of as rungs on a ladder (known as base pairs) involving the pairing of four nucleotides - Adenine (A), Cytosine (C),Guanine (G) and Thymine (T).
- For reference, the SARS-CoV2 virus is approximately 30,0000 base pairs (bp’s), whereas the human genome is approximately 3 billion bp.
:::
:::
:::
::: {.column width="20%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 1: DNA Base Pairs; [@NHGRI2024]](images/DNA_diagram.png)
:::
:::
:::

## DNA Sequencing Technology

::: columns
::: {.column width="50%"}
::: incremental
::: {style="font-size: 0.6em;"}
- Big Picture, the workflow for sequencing is the same for each technology:
    - Someone gets tested for Covid, PCA is ran to detect if the assay is indeed positive
    - Positive tests sequence using a machine that is most likely either Ilumina or Oxford Nanopore machines
    - This takes in the Covid Sample and arrives at a digital copy of DNA sequences
:::
:::
:::
::: {.column width="50%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 2: Simplified Schema of Sequencing Process for SARS-CoV-2](images/DNA_sequencing.png)
:::
:::
:::


## SARS-CoV-2 and Proteins

::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- SARS-CoV-2 has 29 proteins.
- Different proteins have different functions.
- Proteins are encoded from different sites of DNA.
- In studying embeddings, tasks have been either **whole genome oriented** or **protein specific**. 
<!--
- In the context of SARS-CoV-2, proteins are responsible for the virus's structure and infectivity.
-  Nonstructural Proteins primarily function in the replication and manipulation of the host cell's environment to favor viral replication.
-->
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 3: SARS-CoV-2 Proteins; Adapted from [@kandwal2023genetic], pg. 99](images/nsp1_str_non_str.png)
:::
:::
:::

## Embeddings

::: incremental
::: {style="font-size: 0.55em;"}
 - An *embedding* is the name for the dense numeric vectors produced by *embedding algorithms*, such as word2vec or GenSLM.
 
 -  "Embedding" generally refers to the embedding vector
 
 -  Embeddings are representation of data in lower-dimensional space that preserves some aspects of the original structure of the data.

- Embedding algorithms exist for languages (i.e., word2vec) as well as genomic sequences, such as GenSLM.

- But why would you ever embed something?
    - In natural language applications, words can be compared mathematically using their numeric representations, similar to comparing vectors in physics.
    - Embedding vectors are generally more computationally efficient.
:::
:::

::: {.notes}
- As an introduction, it will be easier to first walk through an example of an Embedding for natural language.
:::
## Word Embeddings 

::: incremental
::: {style="font-size: 0.6em;"}
- Before we introduce GenSLM, lets look at a simpler application in natural language.
    - If we consider English text, **How can we measure the similarity between words?**
    - For example, what is the semantic difference between the words "King" and "Woman".
    - Let's demonstrate the semantic relationships of “King”, “Queen”, “Man”, “Woman”.
:::
:::

::: {.notes}
- Embedding algorithms need data (a corpus of text) to construct numeric vectors relating words and their meaning.
:::


## Word2vec Example

. . .

```{webr-r}
#| context: setup

options(max.print = 15)
# Download a dataset
url <- "https://raw.githubusercontent.com/DHintz137/Embedding_Presentation/main/raw-data/King_Man_Woman_Queen_pred.csv"
download.file(url, "King_Man_Woman_Queen_pred.csv")
embedding_raw <- read.csv("King_Man_Woman_Queen_pred.csv")
embedding <- t(data.frame(embedding_raw[,-1] ,row.names = embedding_raw[,1]))
colnames(embedding) <- c("King", "Man", "Woman", "Queen")
# embedding <- as_tibble(embedding)

print_tidy <- function(x, ...) {
  if (is.matrix(x) || length(dim(x)) > 1) {
    # If x is a matrix or array, convert each column into a tibble column
    out <- as_tibble(as.data.frame(x))
  } else if (is.vector(x)) {
    # If x is a vector, convert it into a single column tibble
    out <- tibble(value = x)
  } else {
    # Fallback for other data types
    out <- as_tibble(x)
  }
  print(out, ...)
}
```


```{r, echo=TRUE, eval=FALSE}
#| class-source: small-code
#| classes: small-code

## Pre-Ran Code ##
library(word2vec)
# using British National Corpus http://vectors.nlpl.eu/repository/
model <- read.word2vec("British_National_Corpus_Vector_size_300_skipgram", normalize = TRUE)
embedding <- predict(model,newdata = c("king_NOUN","man_NOUN","woman_NOUN","queen_NOUN"),type="embedding")
```


::: {style="margin-top: 20px;"}
:::

. . .

```{webr-r}
#| class-source: small-code
#| classes: small-code
print_tidy(embedding)
print_tidy(embedding[ ,"King"] - embedding[ , "Man"] + embedding[ , "Woman"])
print_tidy(t(embedding[ , "Man"]) %*% embedding[ , "Woman"])
```

## Plotting Word Embeddings

. . .

::: {style="font-size: 0.6em;"}
- We can Plot the vectors shown in the previous slide to demonstrate how word2vec produces embeddings that measures semantic similarity between words. 
:::

. . .

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 4: Word Embedding Vectors ](images/King_Man_Woman_Queen.png){width="70%"}
:::


## Sequence Embeddings

::: incremental
::: {style="font-size: 0.6em;"}
- Neural network embedding algorithms for genomic data include **GenSLM (the focus of this presentation)**, DNABERT2, and HyenaDNA.
- These embeddings are lossy encodings, meaning that some information is lost in the transformation.
- They also can distort structural relationships present in the data.
:::
:::

::: {.notes}
- Just like we can represent words as vectors we can also represent genomic sequences as vectors.
:::

## Why Data Representation Matters 

::: incremental
::: {style="font-size: 0.6em;"}
- An embedding is one way of representing data.
:::
:::

. . .

::: {style="text-align:center; font-size: 0.5em;"}
![](images/pred_rep_obj.png){width="65%"}
:::

::: {style="margin-top: 20px;"}
:::

::: incremental
::: {style="font-size: 0.6em;"}
-  An effective representation (embedding) of a genomic sequence is crucial for all downstream tasks.
    - i.e., clustering, classification, regression, protein function identification, structural analysis, and predicting genetic disorders [@angermueller2016deep].
- Given the increasing use of embeddings, embedding evolution has also become more important. 
:::
:::

## GenSLM

::: panel-tabset
### Overview

::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- Overall, the GenSLM algorithm first **tokenizes**, i.e., breaks up sequences in to chunks of three nucleotides. 
- Then, the input sequence are **vectorized** creating a $1 \times 512$ vector for each inputed sequence.
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 5: Generic Embedding Workflow](images/Tokenize_vectorize.png){width="50%"}
:::
:::
:::

### Transformer
::: columns
::: {.column width="40%"}
::: incremental
::: {style="font-size: 0.6em;"}
- First the tokenized input sequence is passed to the transformer encoder
- The transformer encoder converts 1,024 bp slices into numeric vectors 
- This is ran recursively through a diffusion model to learn a condensed distribution of the whole sequence.
- The transformers in GenSLM are used to capture local interactions within a genomic sequence
:::
:::
:::
::: {.column width="60%"}
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 6: GenSLM Transformer Architecture; [@genslms2023]](images/GenSLM.png)
:::
:::
:::

### Diffusion  + Transformer



::: incremental
::: {style="font-size: 0.6em;"}
- Diffusion and Transformer models work in tandem in GenSLM
- Transformers captures local interactions, while the diffusion model captures learns a representation of transformers vectors to represent the whole genome
:::
:::


:::

# [Data and Processing]{style="color:white;"}{background-color="#8FA9C2"}


## Data Workflow 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 7: Project Workflow](images/Overall_workflow.png){width="30%"}
:::

## Data Description


::: incremental
::: {style="font-size: 0.6em;"}
- All Covid sequences were downloaded with permission from the Global Initiative on Sharing All Influenza Data (GISAID)
- No geographical or temporal restrictions were placed on the sequences extracted
:::
:::

## Data Cleaning 

::: incremental
::: {style="font-size: 0.6em;"}
- GISAID's exclusion criteria was used to remove sequences of poor quality 
- Additional exclusion criteria was implemented to remove all sequence with ambiguous nucleotides (the `NA's` of the genomic world)
:::
:::


## Alignment 

::: incremental
::: {style="font-size: 0.6em;"}
- An multiple alignment alignment was performed to be able to slice the exact location of proteins across different sequences, for more details see the appendix
:::
::: 

# [Exploratory Data Analysis]{style="color:white;"}{background-color="#8FA9C2"}

## [What are the Patterns and Structures Oberved from the Sequence Data?]{style="font-size: 52px;"}

::: incremental
::: {style="font-size: 0.6em;"}
- For Exploratory Data Analysis, the goal was the characterize the structure of the data in its sequence presentation. 
-  This served as a later comparisons to embedding structure. 
:::
::: 

. . .

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 8: Mutation Boxplot](images/Whole_Genome_Mutation_Count_Hamming_Distances.png){width="50%"}
:::

## 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 11: Hamming Distance of Aligned Proteins to Wuhan Reference Sequence](images/hamming_prot_heatmap_581_all_variants.png){width="100%"}
:::



# [Methodology]{style="color:white;"}{background-color="#8FA9C2"}

## Evaluating Embeddings 

::: incremental
::: {style="font-size: 0.6em;"}
- Broadly speaking, the quality of an embedding is assessed based upon its information richness and the degree of non-redundancy.

- Their are two main methodologies for evaluating embeddings. 
    - When downstream learning tasks are evaluated, it’s referred to as **extrinsic evaluation**.
    - When the qualities of the embedding matrix itself are assessed it is referred to as **intrinsic evaluation** (Lavrač et al., 2021).
:::
:::

## Intrinsic Evaluation

::: incremental
::: {style="font-size: 0.6em;"}
- There are three main qualities to asses when studying the the quality of an embedding: **Redundancy**, **Separability**, **Preservation of Semantic Distance**
    - Redundancy reveals the efficiency of a embeddings encoding; more efficient encodings tend to perform better and use fewer computational resources.
    - Separability gives practical insight into whether or not the embedding output can be separated into meaningful genomic groups (i.e., variants). 
    - Preservation of semantic distance is important for determining if the structural representation of information remains valid for subsequent tasks.
:::
:::

## Extrinsic Evaluation

::: incremental
::: {style="font-size: 0.6em;"}
- Extrinsic evaluation pursues practical benchmarks to assess the performance of an embeddings algorithm per its associated embedding matrix across varying levels of task complexity.
- For GenSLM, the subset of possible tasks chosen were variant and protein classification.
:::
:::

# [Results]{style="color:white;"}{background-color="#8FA9C2"}

## [Intrinsic Evaluation Results]{style="color:#6FB1D1;"}

### [Redundancy]{style="color:#A9D0E3;"}

#### Singular Value Decomposition

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 13: SVD CPE Plot](images/SVD_582.png){width="65%"}
:::

### [Preservation of Semantic Distance]{style="color:#A9D0E3;"}

#### Distance Matrices

::: panel-tabset
##### Sequence 
::: {style="margin-top: 50px;"}
:::

<iframe class="plotlyFrame" src="./seq_dist_matrix.html" style="border: none;"></iframe>

##### Embeddings 
::: {style="margin-top: 50px;"}
:::

<iframe class="plotlyFrame" src="./emb_dist_matrix.html" style="border: none;"></iframe>

##### Abs. Diff.
::: {style="margin-top: 50px;"}
:::

<iframe class="plotlyFrame" src="./abs_diff_dist_matrix.html" style="border: none;"></iframe>

##### Reg. Diff.
::: {style="margin-top: 50px;"}
:::

<iframe class="plotlyFrame" src="./diff_dist_matrix.html" style="border: none;"></iframe>

:::


#### Kullback–Leibler Divergence (KLD)
::: {style="text-align:center; font-size: 0.5em;"}
![Figure 15: CKL vs DKL](images/CKL_vs_DKL_ver2.png){width="85%"}
:::


#### Radial Dendrograms

## 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 12: Aligned Sequence Radial Dendrogram](images/high_res_tree_595_patients_gapinc_cropped.png){width="65%"}
:::

## 

::: {style="text-align:center; font-size: 0.5em;"}
![Figure 16: Embedding Radial Dendrogram](images/high_res_tree_581_patients_emb_cropped.png){width="65%"}
:::

### [Separability]{style="color:#A9D0E3;"}

### Principal Component Analysis

::: {style="margin-top: 70px;"}
:::

<iframe class="plotlyFrame" src="./v_pc_plotly.html" style="border: none;"></iframe>


## [Extrinsic Evaluation Results]{style="color:#6FB1D1;"}

### [Variant Classification]{style="color:#A9D0E3;"}

#### Variant Classification (Embedding)

{{< pdf images/variant_CART.gv.pdf width=1000 height=650 >}}

#### Variant Classification (One-Hot-Encoding)

{{< pdf images/CART_variant_binary_cropped.pdf width=1000 height=650 >}}

### [Protein Classification]{style="color:#A9D0E3;"}

#### Protein Classification (Embedding)

{{< pdf images/protein_CART.gv.pdf width=1000 height=600 >}}

#### Protein Classification (PCA Reduced Embedding)
{{< pdf images/protein_CART_29_PCA_components.gv.pdf width=1000 height=650 >}}


## Conclusion

# [Appendix]{style="color:white;"}{background-color="#8FA9C2"}

## Alignment Illustration

```{webr-r}
#| context: setup
options(max.print = 10)
# Download a dataset
url <- "https://raw.githubusercontent.com/DHintz137/Embedding_Presentation/main/raw-data/var_seq_595_A.feather"
download.file(url, "var_seq_595_A.feather")
var_seq_595_A <- read_feather("var_seq_595_A.feather")
```


::: {style="text-align:center; font-size: 0.5em;"}
![Figure 9: Alignment Example](images/alignment.png){width="65%"}
:::


#

{{< pdf DHintz_PlanB_Submission.pdf width=1000 height=650 >}}

# `r fontawesome::fa("hand-holding-heart", "white")` [Acknowledgements]{style="color:white; font-size: 70px;"} {background-color="#8FA9C2"}

::: incremental
- Committee members 
- Cohort
:::

# [Thank you!!]{.center style="color:white;"}{background-color="#8FA9C2"}

# [References]{.center style="color:#464D58;"}{background-color="#8FA9C2"}

